{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-06-14T14:42:44.904218Z","iopub.status.busy":"2024-06-14T14:42:44.903384Z","iopub.status.idle":"2024-06-14T14:42:47.125021Z","shell.execute_reply":"2024-06-14T14:42:47.124185Z","shell.execute_reply.started":"2024-06-14T14:42:44.904180Z"},"trusted":true},"outputs":[],"source":["import os\n","import numpy as np\n","import cv2\n","from glob import glob\n","import torch\n","import matplotlib.pyplot as plt\n","import gc\n","import h5py\n","import torch\n","from collections import defaultdict\n","from tqdm import tqdm\n","from copy import deepcopy\n","import warnings\n","from functools import partial\n","import fastprogress\n","from tqdm import tqdm as progress_bar\n","import time\n","import random \n","from collections import defaultdict\n","\n","if not torch.cuda.is_available():\n","    print('You may want to enable the GPU switch?')\n","\n","INSTALLED_LOG = {}\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-06-14T14:43:01.469837Z","iopub.status.busy":"2024-06-14T14:43:01.469316Z","iopub.status.idle":"2024-06-14T14:43:33.382030Z","shell.execute_reply":"2024-06-14T14:43:33.380985Z","shell.execute_reply.started":"2024-06-14T14:43:01.469806Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Processing /kaggle/input/kornia-loftr/kornia-0.6.4-py2.py3-none-any.whl\n","Requirement already satisfied: torch>=1.8.1 in /opt/conda/lib/python3.10/site-packages (from kornia==0.6.4) (2.1.2)\n","Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from kornia==0.6.4) (21.3)\n","Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.1->kornia==0.6.4) (3.13.1)\n","Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.1->kornia==0.6.4) (4.9.0)\n","Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.1->kornia==0.6.4) (1.12.1)\n","Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.1->kornia==0.6.4) (3.2.1)\n","Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.1->kornia==0.6.4) (3.1.2)\n","Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.1->kornia==0.6.4) (2024.3.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->kornia==0.6.4) (3.1.1)\n","Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.8.1->kornia==0.6.4) (2.1.3)\n","Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.8.1->kornia==0.6.4) (1.3.0)\n","Installing collected packages: kornia\n","  Attempting uninstall: kornia\n","    Found existing installation: kornia 0.7.2\n","    Uninstalling kornia-0.7.2:\n","      Successfully uninstalled kornia-0.7.2\n","Successfully installed kornia-0.6.4\n","Processing /kaggle/input/kornia-loftr/kornia_moons-0.1.9-py3-none-any.whl\n","Requirement already satisfied: kornia in /opt/conda/lib/python3.10/site-packages (from kornia-moons==0.1.9) (0.6.4)\n","Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from kornia-moons==0.1.9) (2.1.2)\n","Requirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (from kornia-moons==0.1.9) (3.7.5)\n","Requirement already satisfied: opencv-python in /opt/conda/lib/python3.10/site-packages (from kornia-moons==0.1.9) (4.10.0.82)\n","Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from kornia->kornia-moons==0.1.9) (21.3)\n","Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->kornia-moons==0.1.9) (3.13.1)\n","Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch->kornia-moons==0.1.9) (4.9.0)\n","Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->kornia-moons==0.1.9) (1.12.1)\n","Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->kornia-moons==0.1.9) (3.2.1)\n","Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->kornia-moons==0.1.9) (3.1.2)\n","Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->kornia-moons==0.1.9) (2024.3.1)\n","Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->kornia-moons==0.1.9) (1.2.0)\n","Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib->kornia-moons==0.1.9) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->kornia-moons==0.1.9) (4.47.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->kornia-moons==0.1.9) (1.4.5)\n","Requirement already satisfied: numpy<2,>=1.20 in /opt/conda/lib/python3.10/site-packages (from matplotlib->kornia-moons==0.1.9) (1.26.4)\n","Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->kornia-moons==0.1.9) (9.5.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->kornia-moons==0.1.9) (3.1.1)\n","Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib->kornia-moons==0.1.9) (2.9.0.post0)\n","Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib->kornia-moons==0.1.9) (1.16.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->kornia-moons==0.1.9) (2.1.3)\n","Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->kornia-moons==0.1.9) (1.3.0)\n","Installing collected packages: kornia-moons\n","Successfully installed kornia-moons-0.1.9\n"]}],"source":["# Install Kornia\n","force_kornialoftr_reinstall = False\n","\n","if 'KorniaLoFTR' not in INSTALLED_LOG or force_kornialoftr_reinstall:\n","    dry_run = False\n","    !pip install ../input/kornia-loftr/kornia-0.6.4-py2.py3-none-any.whl\n","    !pip install ../input/kornia-loftr/kornia_moons-0.1.9-py3-none-any.whl\n","    INSTALLED_LOG['KorniaLoFTR'] = True\n","else:\n","    print('Already installed KorniaLoFTR. Set \"force_kornialoftr_reinstall=True\" to override this behavior.')\n","    \n","\n","# Import and use Kornia\n","import kornia\n","import kornia as K\n","import kornia.feature as KF\n","import kornia_moons.feature as KMF\n","\n","\n","class LoFTRMatcher:\n","    def __init__(self, device=None, input_longside=1200, conf_th=None):\n","        self._loftr_matcher = KF.LoFTR(pretrained=None)\n","        self._loftr_matcher.load_state_dict(torch.load(\"../input/kornia-loftr/loftr_outdoor.ckpt\")['state_dict'])\n","        self._loftr_matcher = self._loftr_matcher.to(device).eval()\n","        self.device = device\n","        self.input_longside = input_longside\n","        self.conf_thresh = conf_th\n","        \n","    # Prepares an image for LoFTR:    \n","    def prep_img(self, img, long_side=1200):\n","        if long_side is not None: # Resizes the image to a specified long_side\n","            scale = long_side / max(img.shape[0], img.shape[1]) \n","            w = int(img.shape[1] * scale)\n","            h = int(img.shape[0] * scale)\n","            img = cv2.resize(img, (w, h))\n","        else:\n","            scale = 1.0\n","\n","        img_ts = K.image_to_tensor(img, False).float() / 255.\n","        img_ts = K.color.bgr_to_rgb(img_ts)\n","        img_ts = K.color.rgb_to_grayscale(img_ts)\n","        return img, img_ts.to(self.device), scale\n","    \n","    \"\"\"\n","    The function tta_rotation_preprocess :\n","    \n","    Test-Time Augmentation is a technique used to enhance the performance \n","    of a model by augmenting the input data during inference.\n","    \n","    is used to augment the input image by rotating it and preparing it for further processing. \n","    It computes both the rotation and inverse rotation matrices, \n","    applies the rotation, and converts the image into a format suitable for a neural network. \n","    \"\"\"\n","    def tta_rotation_preprocess(self, img_np, angle):\n","        \n","        # Performs Test-Time Augmentation (TTA) with image rotation for a given angle:\n","        \n","        # Computes the rotation matrix.\n","        \n","        rot_M = cv2.getRotationMatrix2D((img_np.shape[1] / 2, img_np.shape[0] / 2), angle, 1)\n","        # Computes the inverse rotation matrix\n","        rot_M_inv = cv2.getRotationMatrix2D((img_np.shape[1] / 2, img_np.shape[0] / 2), -angle, 1)\n","        # Applies the rotation to the image\n","        rot_img = cv2.warpAffine(img_np, rot_M, (img_np.shape[1], img_np.shape[0]))\n","\n","        # Converts the rotated image to a tensor.\n","        rot_img_ts = K.image_to_tensor(rot_img, False).float() / 255.\n","        rot_img_ts = K.color.bgr_to_rgb(rot_img_ts)\n","        rot_img_ts = K.color.rgb_to_grayscale(rot_img_ts)\n","        return rot_M, rot_img_ts.to(self.device), rot_M_inv\n","\n","    \"\"\"\n","    The purpose of this function is to correct the locations of keypoints detected \n","    in a rotated image by transforming them back to their positions in the original image. \n","    This is achieved by applying the inverse of the rotation matrix used during preprocessing. \n","    \n","    Additionally, the function creates a mask to identify keypoints \n","    that remain within the image boundaries after the inverse transformation. \n","    This process ensures that keypoints are accurately mapped back to their original positions, \n","    which is essential for subsequent image matching or processing tasks.\n","    \"\"\"\n","    def tta_rotation_postprocess(self, kpts, img_np, rot_M_inv):\n","        \n","        # Adjusts keypoint locations after applying rotation TTA:\n","        ones = np.ones(shape=(kpts.shape[0], ), dtype=np.float32)[:, None]\n","        hom = np.concatenate([kpts, ones], 1)\n","        rot_kpts = rot_M_inv.dot(hom.T).T[:, :2]\n","        mask = (rot_kpts[:, 0] >= 0) & (rot_kpts[:, 0] < img_np.shape[1]) & (rot_kpts[:, 1] >= 0) & (rot_kpts[:, 1] < img_np.shape[0])\n","        return rot_kpts, mask\n","\n","    # This is the main function for performing feature matching between two images using LoFTR with optional TTA:\n","    def __call__(self, img_np1, img_np2, tta=['orig']):\n","        # tta: List of TTA methods to be applied.\n","        with torch.no_grad():\n","            img_np1, img_ts0, scale0 = self.prep_img(img_np1, long_side=self.input_longside)\n","            img_np2, img_ts1, scale1 = self.prep_img(img_np2, long_side=self.input_longside)\n","            images0, images1 = [], []\n","\n","            # Apply TTA (Test-Time Augmentation):\n","            for tta_elem in tta:\n","                if tta_elem == 'orig':\n","                    img_ts0_aug, img_ts1_aug = img_ts0, img_ts1\n","                elif tta_elem == 'flip_lr':\n","                    img_ts0_aug = torch.flip(img_ts0, [3, ])\n","                    img_ts1_aug = torch.flip(img_ts1, [3, ])\n","                elif tta_elem == 'flip_ud':\n","                    img_ts0_aug = torch.flip(img_ts0, [2, ])\n","                    img_ts1_aug = torch.flip(img_ts1, [2, ])\n","                elif tta_elem == 'rot_r10':\n","                    rot_r10_M0, img_ts0_aug, rot_r10_M0_inv = self.tta_rotation_preprocess(img_np1, 10)\n","                    rot_r10_M1, img_ts1_aug, rot_r10_M1_inv = self.tta_rotation_preprocess(img_np2, 10)\n","                elif tta_elem == 'rot_l10':\n","                    rot_l10_M0, img_ts0_aug, rot_l10_M0_inv = self.tta_rotation_preprocess(img_np1, -10)\n","                    rot_l10_M1, img_ts1_aug, rot_l10_M1_inv = self.tta_rotation_preprocess(img_np2, -10)\n","                else:\n","                    raise ValueError('Unknown TTA method.')\n","                images0.append(img_ts0_aug)\n","                images1.append(img_ts1_aug)\n","\n","            # Inference with LoFTR:\n","            input_dict = {\"image0\": torch.cat(images0), \"image1\": torch.cat(images1)}\n","            correspondences = self._loftr_matcher(input_dict)\n","            mkpts0 = correspondences['keypoints0'].cpu().numpy()\n","            mkpts1 = correspondences['keypoints1'].cpu().numpy()\n","            batch_id = correspondences['batch_indexes'].cpu().numpy()\n","            confidence = correspondences['confidence'].cpu().numpy()\n","\n","            # Reverse TTA Adjustments:\n","            for idx, tta_elem in enumerate(tta):\n","                batch_mask = batch_id == idx\n","\n","                if tta_elem == 'orig':\n","                    pass\n","                elif tta_elem == 'flip_lr':\n","                    mkpts0[batch_mask, 0] = img_np1.shape[1] - mkpts0[batch_mask, 0]\n","                    mkpts1[batch_mask, 0] = img_np2.shape[1] - mkpts1[batch_mask, 0]\n","                elif tta_elem == 'flip_ud':\n","                    mkpts0[batch_mask, 1] = img_np1.shape[0] - mkpts0[batch_mask, 1]\n","                    mkpts1[batch_mask, 1] = img_np2.shape[0] - mkpts1[batch_mask, 1]\n","                elif tta_elem == 'rot_r10':\n","                    mkpts0[batch_mask], mask0 = self.tta_rotation_postprocess(mkpts0[batch_mask], img_np1, rot_r10_M0_inv)\n","                    mkpts1[batch_mask], mask1 = self.tta_rotation_postprocess(mkpts1[batch_mask], img_np2, rot_r10_M1_inv)\n","                    confidence[batch_mask] += (~(mask0 & mask1)).astype(np.float32) * -10.\n","                elif tta_elem == 'rot_l10':\n","                    mkpts0[batch_mask], mask0 = self.tta_rotation_postprocess(mkpts0[batch_mask], img_np1, rot_l10_M0_inv)\n","                    mkpts1[batch_mask], mask1 = self.tta_rotation_postprocess(mkpts1[batch_mask], img_np2, rot_l10_M1_inv)\n","                    confidence[batch_mask] += (~(mask0 & mask1)).astype(np.float32) * -10.\n","                else:\n","                    raise ValueError('Unknown TTA method.')\n","            \n","            # Filter Keypoints by Confidence Threshold:\n","            if self.conf_thresh is not None:\n","                th_mask = confidence >= self.conf_thresh\n","            else:\n","                th_mask = confidence >= 0.\n","            mkpts0, mkpts1 = mkpts0[th_mask, :], mkpts1[th_mask, :]\n","\n","            # Matching points\n","            return mkpts0 / scale0, mkpts1 / scale1\n","        \n","\n","# 1200 is the validation size, according to the paper\n","\n","loftr_matcher = LoFTRMatcher(device=device, input_longside=1200, conf_th=0.9)"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-06-14T14:43:33.384465Z","iopub.status.busy":"2024-06-14T14:43:33.384026Z","iopub.status.idle":"2024-06-14T14:43:38.996371Z","shell.execute_reply":"2024-06-14T14:43:38.995264Z","shell.execute_reply.started":"2024-06-14T14:43:33.384438Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["__init__.py  matching.py  superglue.py\tsuperpoint.py  utils.py  weights\n","Loaded SuperPoint model\n","Loaded SuperGlue model (\"outdoor\" weights)\n"]}],"source":["# Install superglue\n","force_superglue_reinstall = False\n","\n","if 'superglue' not in INSTALLED_LOG or force_superglue_reinstall:\n","    !mkdir /tmp/superpoint\n","    !cp -r ../input/super-glue-pretrained-network/models /tmp/superpoint/superpoint\n","    !ls /tmp/superpoint/superpoint\n","    !touch /tmp/superpoint/superpoint/__init__.py\n","    INSTALLED_LOG['superglue'] = True\n","else:\n","    print('Already installed SuperGlue. Set \"force_superglue_reinstall=True\" to override this behavior.')\n","\n","# Import superglue\n","import sys\n","sys.path.append(\"/tmp/superpoint\")\n","from superpoint.superpoint import SuperPoint\n","from superpoint.superglue import SuperGlue\n","\n","\n","class SuperGlueCustomMatchingV2(torch.nn.Module):\n","    \"\"\" Image Matching Frontend (SuperPoint + SuperGlue) \"\"\"\n","    def __init__(self, config={}, device=None):\n","        super().__init__()\n","        self.superpoint = SuperPoint(config.get('superpoint', {}))\n","        self.superglue = SuperGlue(config.get('superglue', {}))\n","\n","        self.tta_map = {\n","            'orig': self.untta_none,\n","            'eqhist': self.untta_none,\n","            'clahe': self.untta_none,\n","            'flip_lr': self.untta_fliplr,\n","            'flip_ud': self.untta_flipud,\n","            'rot_r10': self.untta_rotr10,\n","            'rot_l10': self.untta_rotl10,\n","            'fliplr_rotr10': self.untta_fliplr_rotr10,\n","            'fliplr_rotl10': self.untta_fliplr_rotl10\n","        }\n","        self.device = device\n","\n","    def forward_flat(self, data, ttas=['orig', ], tta_groups=[['orig']]):\n","        \"\"\" Run SuperPoint (optionally) and SuperGlue\n","        SuperPoint is skipped if ['keypoints0', 'keypoints1'] exist in input\n","        Args:\n","          data: dictionary with minimal keys: ['image0', 'image1']\n","        \"\"\"\n","        # 1. Initialization:\n","        pred = {}\n","\n","        # 2. Extract SuperPoint (keypoints, scores, descriptors) if not provided\n","        # sp_st = time.time()\n","        if 'keypoints0' not in data:\n","            pred0 = self.superpoint({'image': data['image0']})\n","            pred = {**pred, **{k+'0': v for k, v in pred0.items()}}\n","        if 'keypoints1' not in data:\n","            pred1 = self.superpoint({'image': data['image1']})\n","            pred = {**pred, **{k+'1': v for k, v in pred1.items()}}\n","        # sp_nd = time.time()\n","        # print('SP:', sp_nd - sp_st, 's')\n","\n","        # 3. Reverse TTA (Test-Time Augmentation):\n","        # The scores from SuperPoint are converted to lists to enable modification:    \n","        pred['scores0'] = list(pred['scores0'])\n","        pred['scores1'] = list(pred['scores1'])\n","        \n","        for i in range(len(pred['keypoints0'])): \n","            # The corresponding TTA transformation function is applied to reverse the transformation\n","            # This includes flipping, rotating, or any other augmentation applied during inference\n","            pred['keypoints0'][i], pred['descriptors0'][i], pred['scores0'][i] = self.tta_map[ttas[i]](\n","                pred['keypoints0'][i], pred['descriptors0'][i], pred['scores0'][i],\n","                # The inplace parameter controls whether the original data is modified or a copy is returned\n","                # Illegal keypoints (outside the image dimensions) can be optionally masked out.\n","                w=data['image0'].shape[3], h=data['image0'].shape[2], inplace=True, mask_illegal=True)\n","                \n","            # This process is repeated for both images\n","            pred['keypoints1'][i], pred['descriptors1'][i], pred['scores1'][i] = self.tta_map[ttas[i]](\n","                pred['keypoints1'][i], pred['descriptors1'][i], pred['scores1'][i],\n","                w=data['image1'].shape[3], h=data['image1'].shape[2], inplace=True, mask_illegal=True)\n","\n","        # 4. Batch all features :\n","        \n","        # We should either have i) one image per batch, or\n","        # ii) the same number of local features for all images in the batch.\n","        data = {**data, **pred}\n","        \n","        # The extracted features (keypoints, descriptors, and scores) are merged with the input data to form a batch.\n","        # This step ensures that the data has consistent dimensions required for further processing.\n","        \n","        # 5. Grouping Predictions:\n","        group_preds = []\n","        for tta_group in tta_groups:\n","            # Create a boolean mask to filter relevant transformations\n","            group_mask = torch.from_numpy(np.array([x in tta_group for x in ttas], dtype=np.bool))\n","            # Select group-specific data based on the mask\n","            group_data = {\n","                **{f'keypoints{k}': [data[f'keypoints{k}'][i] for i in range(len(ttas)) if ttas[i] in tta_group] for k in [0, 1]},\n","                **{f'descriptors{k}': [data[f'descriptors{k}'][i] for i in range(len(ttas)) if ttas[i] in tta_group] for k in [0, 1]},\n","                **{f'scores{k}': [data[f'scores{k}'][i] for i in range(len(ttas)) if ttas[i] in tta_group] for k in [0, 1]},\n","                **{f'image{k}': data[f'image{k}'][group_mask, ...] for k in [0, 1]},\n","            }\n","            \n","            # Concatenate data along appropriate axis if it's in list/tuple format\n","            for k, v in group_data.items():\n","                if isinstance(group_data[k], (list, tuple)):\n","                    if k.startswith('descriptor'):\n","                        group_data[k] = torch.cat(group_data[k], 1)[None, ...]\n","                    else:\n","                        group_data[k] = torch.cat(group_data[k])[None, ...]\n","                else:\n","                    group_data[k] = torch.flatten(group_data[k], 0, 1)[None, ...]\n","            # sg_st = time.time()\n","            \n","            # Pass the grouped data through the SuperGlue module to compute matches and matching scores\n","            group_pred = {\n","                # **{k: group_data[k] for k in group_data},\n","                **group_data,\n","                **self.superglue(group_data)\n","            }\n","            # sg_nd = time.time()\n","            # print('SG:', sg_nd - sg_st, 's')\n","            group_preds.append(group_pred)\n","        return group_preds\n","\n","    def forward_cross(self, data, ttas=['orig', ], tta_groups=[('orig', 'orig')]):\n","        pred = {}\n","\n","        # Extract SuperPoint (keypoints, scores, descriptors) if not provided\n","        sp_st = time.time()\n","        if 'keypoints0' not in data:\n","            pred0 = self.superpoint({'image': data['image0']})\n","            pred = {**pred, **{k+'0': v for k, v in pred0.items()}}\n","        if 'keypoints1' not in data:\n","            pred1 = self.superpoint({'image': data['image1']})\n","            pred = {**pred, **{k+'1': v for k, v in pred1.items()}}\n","        sp_nd = time.time()\n","\n","        # Batch all features\n","        # We should either have i) one image per batch, or\n","        # ii) the same number of local features for all images in the batch.\n","        data = {**data, **pred}\n","\n","        # Group predictions (list, with elements with matches{0,1}, matching_scores{0,1} keys)\n","        group_pred_list = []\n","        tta2id = {k: i for i, k in enumerate(ttas)}\n","        for tta_group in tta_groups:\n","            group_idx = tta2id[tta_group[0]], tta2id[tta_group[1]]\n","            group_data = {\n","                **{f'image{i}': data[f'image{i}'][group_idx[i]:group_idx[i]+1] for i in [0, 1]},\n","                **{f'keypoints{i}': data[f'keypoints{i}'][group_idx[i]:group_idx[i]+1] for i in [0, 1]},\n","                **{f'descriptors{i}': data[f'descriptors{i}'][group_idx[i]:group_idx[i]+1] for i in [0, 1]},\n","                **{f'scores{i}': data[f'scores{i}'][group_idx[i]:group_idx[i]+1] for i in [0, 1]},\n","            }\n","\n","            for k in group_data:\n","                if isinstance(group_data[k], (list, tuple)):\n","                    group_data[k] = torch.stack(group_data[k])\n","\n","            group_sg_pred = self.superglue(group_data)\n","            group_pred_list.append(group_sg_pred)\n","\n","        # UnTTA\n","        data['scores0'] = list(data['scores0'])\n","        data['scores1'] = list(data['scores1'])\n","        for i in range(len(data['keypoints0'])):\n","            data['keypoints0'][i], data['descriptors0'][i], data['scores0'][i] = self.tta_map[ttas[i]](\n","                data['keypoints0'][i], data['descriptors0'][i], data['scores0'][i],\n","                w=data['image0'].shape[3], h=data['image0'].shape[2], inplace=True, mask_illegal=False)\n","\n","            data['keypoints1'][i], data['descriptors1'][i], data['scores1'][i] = self.tta_map[ttas[i]](\n","                data['keypoints1'][i], data['descriptors1'][i], data['scores1'][i],\n","                w=data['image1'].shape[3], h=data['image1'].shape[2], inplace=True, mask_illegal=False)\n","\n","        # Sooo... groups?\n","        for group_pred, tta_group in zip(group_pred_list, tta_groups):\n","            group_idx = tta2id[tta_group[0]], tta2id[tta_group[1]]\n","            group_pred.update({\n","                **{f'keypoints{i}': data[f'keypoints{i}'][group_idx[i]:group_idx[i]+1] for i in [0, 1]},\n","                **{f'scores{i}': data[f'scores{i}'][group_idx[i]:group_idx[i]+1] for i in [0, 1]},\n","            })\n","        return group_pred_list\n","\n","\n","    def untta_none(self, keypoints, descriptors, scores, w, h, inplace=True, mask_illegal=True):\n","        if not inplace:\n","            keypoints = keypoints.clone()\n","        return keypoints, descriptors, scores\n","    \n","    def untta_fliplr(self, keypoints, descriptors, scores, w, h, inplace=True, mask_illegal=True):\n","        # It horizontally flips the keypoints. \n","        # It subtracts each x-coordinate of the keypoints from the width of the image minus 1 to simulate a horizontal flip.\n","        if not inplace:\n","            keypoints = keypoints.clone()\n","        keypoints[:, 0] = w - keypoints[:, 0] - 1.\n","        return keypoints, descriptors, scores\n","\n","    def untta_flipud(self, keypoints, descriptors, scores, w, h, inplace=True, mask_illegal=True):\n","        # It vertically flips the keypoints. \n","        # It subtracts each y-coordinate of the keypoints from the height of the image minus 1 to simulate a vertical flip.\n","        if not inplace:\n","            keypoints = keypoints.clone()\n","        keypoints[:, 1] = h - keypoints[:, 1] - 1.\n","        return keypoints, descriptors, scores\n","\n","    def untta_rotr10(self, keypoints, descriptors, scores, w, h, inplace=True, mask_illegal=True):\n","        # It rotates the keypoints clockwise by 10 degrees around the center of the image. \n","        # It uses a rotation matrix to perform the rotation.\n","        # rotr10 is +10, inverse is -10\n","        rot_M_inv = torch.from_numpy(cv2.getRotationMatrix2D((w / 2, h / 2), -15, 1)).to(torch.float32).to(self.device)\n","        ones = torch.ones_like(keypoints[:, 0])\n","        hom = torch.cat([keypoints, ones[:, None]], 1)\n","        rot_kpts = torch.matmul(rot_M_inv, hom.T).T[:, :2]\n","        if mask_illegal:\n","            mask = (rot_kpts[:, 0] >= 0) & (rot_kpts[:, 0] < w) & (rot_kpts[:, 1] >= 0) & (rot_kpts[:, 1] < h)\n","            return rot_kpts[mask], descriptors[:, mask], scores[mask]\n","        else:\n","            return rot_kpts, descriptors, scores\n","\n","    def untta_rotl10(self, keypoints, descriptors, scores, w, h, inplace=True, mask_illegal=True):\n","        #  It rotates the keypoints counterclockwise by 10 degrees around the center of the image. \n","        # It also uses a rotation matrix to perform the rotation.\n","        # rotr10 is -10, inverse is +10\n","        rot_M_inv = torch.from_numpy(cv2.getRotationMatrix2D((w / 2, h / 2), 15, 1)).to(torch.float32).to(self.device)\n","        ones = torch.ones_like(keypoints[:, 0])\n","        hom = torch.cat([keypoints, ones[:, None]], 1)\n","        rot_kpts = torch.matmul(rot_M_inv, hom.T).T[:, :2]\n","        if mask_illegal:\n","            mask = (rot_kpts[:, 0] >= 0) & (rot_kpts[:, 0] < w) & (rot_kpts[:, 1] >= 0) & (rot_kpts[:, 1] < h)\n","            return rot_kpts[mask], descriptors[:, mask], scores[mask]\n","        else:\n","            return rot_kpts, descriptors, scores\n","        \n","    def untta_fliplr_rotr10(self, keypoints, descriptors, scores, w, h, inplace=True, mask_illegal=True):\n","        # It first horizontally flips the keypoints and then rotates them clockwise by 10 degrees. \n","        # It combines the transformations by applying them sequentially.\n","        # rotr10 is +10, inverse is -10\n","        rot_M_inv = torch.from_numpy(cv2.getRotationMatrix2D((w / 2, h / 2), -15, 1)).to(torch.float32).to(self.device)\n","        ones = torch.ones_like(keypoints[:, 0])\n","        hom = torch.cat([keypoints, ones[:, None]], 1)\n","        rot_kpts = torch.matmul(rot_M_inv, hom.T).T[:, :2]\n","        rot_kpts[:, 0] = w - rot_kpts[:, 0] - 1.\n","        if mask_illegal:\n","            mask = (rot_kpts[:, 0] >= 0) & (rot_kpts[:, 0] < w) & (rot_kpts[:, 1] >= 0) & (rot_kpts[:, 1] < h)\n","            return rot_kpts[mask], descriptors[:, mask], scores[mask]\n","        else:\n","            return rot_kpts, descriptors, scores\n","\n","    def untta_fliplr_rotl10(self, keypoints, descriptors, scores, w, h, inplace=True, mask_illegal=True):\n","        #  It first horizontally flips the keypoints and then rotates them counterclockwise by 10 degrees. \n","        # Similar to the previous function, it combines the transformations by applying them sequentially.\n","        # rotr10 is -10, inverse is +10\n","        rot_M_inv = torch.from_numpy(cv2.getRotationMatrix2D((w / 2, h / 2), 15, 1)).to(torch.float32).to(self.device)\n","        ones = torch.ones_like(keypoints[:, 0])\n","        hom = torch.cat([keypoints, ones[:, None]], 1)\n","        rot_kpts = torch.matmul(rot_M_inv, hom.T).T[:, :2]\n","        rot_kpts[:, 0] = w - rot_kpts[:, 0] - 1.\n","        if mask_illegal:\n","            mask = (rot_kpts[:, 0] >= 0) & (rot_kpts[:, 0] < w) & (rot_kpts[:, 1] >= 0) & (rot_kpts[:, 1] < h)\n","            return rot_kpts[mask], descriptors[:, mask], scores[mask]\n","        else:\n","            return rot_kpts, descriptors, scores\n","\n","\n","class SuperGlueMatcherV2:\n","    def __init__(self, device=None, conf_th=None):\n","        config = {\n","            \"superpoint\": {\n","                \"nms_radius\": 2,  # Reduce the NMS radius to filter out less distinctive keypoints\n","                \"keypoint_threshold\": 0.2,  # Lower the keypoint detection threshold to capture only highly distinctive keypoints\n","                \"max_keypoints\": 3000,  # Decrease the maximum number of keypoints to focus on the most prominent features\n","            },\n","            \"superglue\": {\n","                \"weights\": \"outdoor\", # Use outdoor weights for better matching of natural scenes\n","                \"sinkhorn_iterations\": 100, # specifies the number of iterations to perform during the Sinkhorn normalization process\n","                \"match_threshold\": 0.2,  # Lower the matching threshold for more conservative matches, focusing on highly confident matches\n","            }\n","        }\n","        self.device = device\n","        self._superglue_matcher = SuperGlueCustomMatchingV2(\n","            config=config, device=self.device,\n","            ).eval().to(device)\n","\n","        self.conf_thresh = conf_th\n","    \n","    def prep_np_img(self, img, long_side=None):\n","        if long_side is not None:\n","            scale = long_side / max(img.shape[0], img.shape[1])\n","            w = int(img.shape[1] * scale)\n","            h = int(img.shape[0] * scale)\n","            img = cv2.resize(img, (w, h))\n","        else:\n","            scale = 1.0\n","        return cv2.cvtColor(img, cv2.COLOR_BGR2GRAY), scale\n","    \n","    def frame2tensor(self, frame):\n","        return (torch.from_numpy(frame).float()/255.)[None, None].to(self.device)\n","            \n","        \n","    def tta_rotation_preprocess(self, img_np, angle):\n","        rot_M = cv2.getRotationMatrix2D((img_np.shape[1] / 2, img_np.shape[0] / 2), angle, 1)\n","        rot_M_inv = cv2.getRotationMatrix2D((img_np.shape[1] / 2, img_np.shape[0] / 2), -angle, 1)\n","        rot_img = self.frame2tensor(cv2.warpAffine(img_np, rot_M, (img_np.shape[1], img_np.shape[0])))\n","        return rot_M, rot_img, rot_M_inv\n","\n","    def tta_rotation_postprocess(self, kpts, img_np, rot_M_inv):\n","        ones = np.ones(shape=(kpts.shape[0], ), dtype=np.float32)[:, None]\n","        hom = np.concatenate([kpts, ones], 1)\n","        rot_kpts = rot_M_inv.dot(hom.T).T[:, :2]\n","        mask = (rot_kpts[:, 0] >= 0) & (rot_kpts[:, 0] < img_np.shape[1]) & (rot_kpts[:, 1] >= 0) & (rot_kpts[:, 1] < img_np.shape[0])\n","        return rot_kpts, mask\n","\n","    def __call__(self, img_np0, img_np1, tta_groups=[['orig']], forward_type='cross', input_longside=None):\n","        with torch.no_grad():\n","            img_np0, scale0 = self.prep_np_img(img_np0, input_longside)\n","            img_np1, scale1 = self.prep_np_img(img_np1, input_longside)\n","\n","            img_ts0 = self.frame2tensor(img_np0)\n","            img_ts1 = self.frame2tensor(img_np1)\n","            images0, images1 = [], []\n","\n","            tta = []\n","            for tta_g in tta_groups:\n","                tta += tta_g\n","            tta = list(set(tta))\n","\n","            # TTA\n","            for tta_elem in tta:\n","                if tta_elem == 'orig':\n","                    img_ts0_aug, img_ts1_aug = img_ts0, img_ts1\n","                elif tta_elem == 'flip_lr':\n","                    img_ts0_aug = torch.flip(img_ts0, [3, ])\n","                    img_ts1_aug = torch.flip(img_ts1, [3, ])\n","                elif tta_elem == 'flip_ud':\n","                    img_ts0_aug = torch.flip(img_ts0, [2, ])\n","                    img_ts1_aug = torch.flip(img_ts1, [2, ])\n","                elif tta_elem == 'rot_r10':\n","                    rot_r10_M0, img_ts0_aug, rot_r10_M0_inv = self.tta_rotation_preprocess(img_np0, 15)\n","                    rot_r10_M1, img_ts1_aug, rot_r10_M1_inv = self.tta_rotation_preprocess(img_np1, 15)\n","                elif tta_elem == 'rot_l10':\n","                    rot_l10_M0, img_ts0_aug, rot_l10_M0_inv = self.tta_rotation_preprocess(img_np0, -15)\n","                    rot_l10_M1, img_ts1_aug, rot_l10_M1_inv = self.tta_rotation_preprocess(img_np1, -15)\n","                elif tta_elem == 'fliplr_rotr10':\n","                    rot_r10_M0, img_ts0_aug, rot_r10_M0_inv = self.tta_rotation_preprocess(img_np0[:, ::-1], 15)\n","                    rot_r10_M1, img_ts1_aug, rot_r10_M1_inv = self.tta_rotation_preprocess(img_np1[:, ::-1], 15)\n","                elif tta_elem == 'fliplr_rotl10':\n","                    rot_l10_M0, img_ts0_aug, rot_l10_M0_inv = self.tta_rotation_preprocess(img_np0[:, ::-1], -15)\n","                    rot_l10_M1, img_ts1_aug, rot_l10_M1_inv = self.tta_rotation_preprocess(img_np1[:, ::-1], -15)\n","                elif tta_elem == 'eqhist':\n","                    img_ts0_aug = self.frame2tensor(cv2.equalizeHist(img_np0))\n","                    img_ts1_aug = self.frame2tensor(cv2.equalizeHist(img_np1))\n","                elif tta_elem == 'clahe':\n","                    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n","                    img_ts0_aug = self.frame2tensor(clahe.apply(img_np0))\n","                    img_ts1_aug = self.frame2tensor(clahe.apply(img_np1))\n","                else:\n","                    raise ValueError('Unknown TTA method.')\n","\n","                images0.append(img_ts0_aug)\n","                images1.append(img_ts1_aug)\n","\n","            # Inference\n","            if forward_type == 'cross':\n","                pred = self._superglue_matcher.forward_cross(\n","                    data={\n","                        \"image0\": torch.cat(images0),\n","                        \"image1\": torch.cat(images1)\n","                    },\n","                    ttas=tta, tta_groups=tta_groups)\n","            elif forward_type == 'flat':\n","                pred = self._superglue_matcher.forward_flat(\n","                data={\n","                    \"image0\": torch.cat(images0),\n","                    \"image1\": torch.cat(images1)\n","                },\n","                ttas=tta, tta_groups=tta_groups)\n","            else:\n","                raise RuntimeError(f'Unknown forward_type {forward_type}')\n","\n","            mkpts0, mkpts1, mconf = [], [], []\n","            for group_pred in pred:\n","                pred_aug = {k: v[0].detach().cpu().numpy().squeeze() for k, v in group_pred.items()}\n","                kpts0, kpts1 = pred_aug[\"keypoints0\"], pred_aug[\"keypoints1\"]\n","                matches, conf = pred_aug[\"matches0\"], pred_aug[\"matching_scores0\"]\n","\n","                if self.conf_thresh is None:\n","                    valid = matches > -1\n","                else:\n","                    valid = (matches > -1) & (conf >= self.conf_thresh)\n","                mkpts0.append(kpts0[valid])\n","                mkpts1.append(kpts1[matches[valid]])\n","                mconf.append(conf[valid])\n","\n","            cat_mkpts0 = np.concatenate(mkpts0)\n","            cat_mkpts1 = np.concatenate(mkpts1)\n","            mask0 = (cat_mkpts0[:, 0] >= 0) & (cat_mkpts0[:, 0] < img_np0.shape[1]) & (cat_mkpts0[:, 1] >= 0) & (cat_mkpts0[:, 1] < img_np0.shape[0])\n","            mask1 = (cat_mkpts1[:, 0] >= 0) & (cat_mkpts1[:, 0] < img_np1.shape[1]) & (cat_mkpts1[:, 1] >= 0) & (cat_mkpts1[:, 1] < img_np1.shape[0])\n","            return cat_mkpts0[mask0 & mask1] / scale0, cat_mkpts1[mask0 & mask1] / scale1\n","\n","\n","# 1600 is the validation size in the paper\n","superglue_matcher = SuperGlueMatcherV2(device=device, conf_th=0.5)\n"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-06-14T14:43:38.999553Z","iopub.status.busy":"2024-06-14T14:43:38.998586Z","iopub.status.idle":"2024-06-14T14:43:39.011265Z","shell.execute_reply":"2024-06-14T14:43:39.010382Z","shell.execute_reply.started":"2024-06-14T14:43:38.999515Z"},"trusted":true},"outputs":[],"source":["warnings.filterwarnings(\"ignore\", category=UserWarning)\n","\n","def get_unique_idxs(A, dim=1):\n","    if A.size(dim) == 0:\n","        return torch.tensor([], dtype=torch.long, device=A.device)\n","    \n","    unique, idx, counts = torch.unique(A, dim=dim, sorted=True, return_inverse=True, return_counts=True)\n","    _, ind_sorted = torch.sort(idx, stable=True)\n","    \n","    if dim == 0:\n","        cum_sum = counts.cumsum(0)\n","        cum_sum = torch.cat((torch.tensor([0], device=cum_sum.device), cum_sum[:-1]))\n","        first_indicies = ind_sorted[cum_sum]\n","    else:  # dim == 1\n","        cum_sum = counts.cumsum(0)\n","        cum_sum = torch.cat((torch.tensor([0], device=cum_sum.device), cum_sum[:-1]))\n","        first_indicies = ind_sorted[cum_sum]\n","    \n","    return first_indicies\n","\n","\n","\n","def match_images(img_path1, img_path2):\n","    img_np1 = cv2.imread(img_path1)\n","    img_np2 = cv2.imread(img_path2)\n","    \n","    matchers_cfg = [\n","        {\n","            'name': 'loftr',\n","            'fn': partial(loftr_matcher, tta=['orig', 'flip_lr']),\n","        },\n","        {\n","            'name': 'superglue',\n","            'fn': partial(superglue_matcher, tta_groups=[\n","                ('orig', 'orig'), ('orig', 'rot_r10'), ('rot_r10', 'orig'), ('flip_lr', 'flip_lr')\n","            ], forward_type='cross', input_longside=1600)\n","        },\n","    ]\n","    \n","    mkpts0, mkpts1, kp_sources = [], [], []\n","\n","    for m_cfg in matchers_cfg:\n","        m_mkpts0, m_mkpts1 = m_cfg['fn'](img_np1, img_np2)\n","        # print(f\"Number of matches found by {m_cfg['name']}: {len(m_mkpts0)}\")\n","        mkpts0.append(m_mkpts0)\n","        mkpts1.append(m_mkpts1)\n","        kp_sources.append([m_cfg['name']] * len(m_mkpts0))\n","    \n","    mkpts0 = np.concatenate(mkpts0)\n","    mkpts1 = np.concatenate(mkpts1)\n","    kp_sources = np.concatenate(kp_sources)\n","\n","    return mkpts0, mkpts1, kp_sources, img_np1, img_np2"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-06-14T14:43:39.960035Z","iopub.status.busy":"2024-06-14T14:43:39.959614Z","iopub.status.idle":"2024-06-14T14:43:39.981894Z","shell.execute_reply":"2024-06-14T14:43:39.980943Z","shell.execute_reply.started":"2024-06-14T14:43:39.960006Z"},"trusted":true},"outputs":[],"source":["# Main script\n","dirname = '/kaggle/input/miawww'\n","img_fnames = [os.path.join(dirname, x) for x in os.listdir(dirname) if x.endswith('.jpg')]\n","\n","pairs_within_dir = [(i, j) for i in range(len(img_fnames)) for j in range(i + 1, len(img_fnames))]\n","\n","feature_dir = '/kaggle/working/'\n","os.makedirs(feature_dir, exist_ok=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-14T14:43:40.796468Z","iopub.status.busy":"2024-06-14T14:43:40.795705Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":[" 57%|█████▋    | 2773/4851 [43:34<32:40,  1.06it/s]  "]}],"source":["# Initialize dictionaries and default dictionaries\n","kpts = defaultdict(list)\n","match_indexes = defaultdict(dict)\n","total_kpts = defaultdict(int)\n","unique_kpts = {}\n","unique_match_idxs = {}\n","out_match = defaultdict(dict)\n","\n","# Iterate over pairs and process\n","for pair_idx in tqdm(pairs_within_dir):\n","    idx1, idx2 = pair_idx\n","    img_path1, img_path2 = img_fnames[idx1], img_fnames[idx2]\n","    img_name1 = os.path.basename(img_path1)  # Extract filename from path\n","    img_name2 = os.path.basename(img_path2)  # Extract filename from path\n","    mkpts0, mkpts1, kp_sources, img_np1, img_np2 = match_images(img_path1, img_path2)\n","\n","    # Get unique keypoints\n","    mkpts0_torch = torch.from_numpy(mkpts0)\n","    mkpts1_torch = torch.from_numpy(mkpts1)\n","    unique_idxs = get_unique_idxs(mkpts0_torch, dim=0)\n","    unique_mkpts0 = mkpts0[unique_idxs.numpy()]\n","    unique_mkpts1 = mkpts1[unique_idxs.numpy()]\n","\n","    # Store unique keypoints in defaultdict\n","    kpts[img_name1].append(unique_mkpts0)\n","    kpts[img_name2].append(unique_mkpts1)\n","\n","    # Store match indices in defaultdict\n","    current_match = torch.arange(len(unique_idxs)).reshape(-1, 1).repeat(1, 2)\n","    current_match[:, 0] += total_kpts[img_name1]\n","    current_match[:, 1] += total_kpts[img_name2]\n","\n","    # Update total keypoints count\n","    total_kpts[img_name1] += len(unique_idxs)\n","    total_kpts[img_name2] += len(unique_idxs)\n","\n","    match_indexes[img_name1][img_name2] = current_match.numpy().tolist()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Convert lists of keypoints to numpy arrays\n","for key in kpts:\n","    kpts[key] = np.concatenate(kpts[key])\n","\n","# Finding Unique Keypoints\n","for k in kpts.keys():\n","    uniq_kps, uniq_reverse_idxs = torch.unique(torch.from_numpy(kpts[k]), dim=0, return_inverse=True)\n","    unique_match_idxs[k] = uniq_reverse_idxs\n","    unique_kpts[k] = uniq_kps.numpy()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Process match indexes to remove duplicates\n","for k1, group in match_indexes.items():\n","    for k2, m in group.items():\n","        m2 = deepcopy(m)\n","        m2 = np.array(m2)  # Convert to numpy array for indexing\n","\n","        if m2.ndim == 1:\n","            m2 = m2.reshape(-1, 2)  # Ensure m2 is 2-dimensional\n","\n","        if m2.shape[0] == 0:\n","            # Skip empty matches\n","            continue\n","\n","        if k1 in unique_match_idxs and k2 in unique_match_idxs:\n","            unique_match_k1 = unique_match_idxs[k1]\n","            unique_match_k2 = unique_match_idxs[k2]\n","        else:\n","            continue  # Skip if unique_match_idxs do not contain required keys\n","\n","        m2[:, 0] = unique_match_k1[np.array(m2[:, 0], dtype=int)]\n","        m2[:, 1] = unique_match_k2[np.array(m2[:, 1], dtype=int)]\n","\n","        mkpts = np.concatenate([unique_kpts[k1][m2[:, 0]], unique_kpts[k2][m2[:, 1]]], axis=1)\n","\n","        # Convert mkpts to PyTorch tensor before passing to get_unique_idxs\n","        mkpts_tensor = torch.from_numpy(mkpts)\n","        unique_idxs_current = get_unique_idxs(mkpts_tensor, dim=0)\n","\n","        # Convert m2 to PyTorch tensor before indexing\n","        m2 = torch.from_numpy(m2)\n","        m2_semiclean = m2[unique_idxs_current]\n","        unique_idxs_current1 = get_unique_idxs(m2_semiclean[:, 0], dim=0)\n","        m2_semiclean = m2_semiclean[unique_idxs_current1]\n","        unique_idxs_current2 = get_unique_idxs(m2_semiclean[:, 1], dim=0)\n","        m2_semiclean2 = m2_semiclean[unique_idxs_current2]\n","        out_match[k1][k2] = m2_semiclean2.tolist()  # Convert back to list for HDF5 saving\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Open an HDF5 file in write mode\n","file_path = \"keypoints.h5\"\n","with h5py.File(file_path, 'w') as f:\n","    try:\n","        for image_path, keypoints_array in unique_kpts.items():\n","            # Extract image file name from full path\n","            image_filename = os.path.basename(image_path)\n","\n","            # Create a dataset named after the image filename\n","            f.create_dataset(image_filename, data=keypoints_array.astype(np.float32))\n","\n","            # Optionally, you can also set attributes if needed:\n","            f[image_filename].attrs['description'] = f'Keypoints for image {image_filename}'\n","\n","            print(f\"Dataset: {image_filename}, Shape: {keypoints_array.shape}, Dtype: {keypoints_array.dtype}\")\n","\n","        print(f'Keypoints saved to {file_path}')\n","    \n","    except Exception as e:\n","        print(f\"Error saving matches to HDF5 file: {e}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Open an HDF5 file in write mode\n","file_path = \"matches.h5\"\n","with h5py.File(file_path, 'w') as f:\n","    for image_filename, keypoints_array in unique_kpts.items():\n","        group = f.create_group(image_filename)\n","        for matched_image_filename, matches in out_match[image_filename].items():\n","            group.create_dataset(matched_image_filename, data=np.array(matches).astype(np.float32))\n","\n","print(f'Matches saved to {file_path}')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def load_image(image_path):\n","    \"\"\"Loads an image from file.\"\"\"\n","    image = cv2.imread(image_path)\n","    if image is None:\n","        raise FileNotFoundError(f\"Image not found at path: {image_path}\")\n","    return cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","\n","def visualize_keypoints(image_dir, keypoints, num_images=5, point_size=1):\n","    sample_keys = random.sample(list(keypoints.keys()), num_images)\n","    fig, axes = plt.subplots(1, num_images, figsize=(50, 10))\n","    for ax, key in zip(axes, sample_keys):\n","        img_path = os.path.join(image_dir, key)\n","        img = load_image(img_path)\n","        kp = keypoints[key]\n","        ax.imshow(img)\n","        ax.scatter(kp[:, 0], kp[:, 1], s=point_size, c='red', marker='o')\n","        ax.axis('off')\n","        ax.set_title(key)  # Add the image name as the title\n","    plt.show()\n","\n","# Load keypoints from the saved file\n","def load_keypoints(feature_dir):\n","    with h5py.File(f'{feature_dir}/keypoints.h5', 'r') as f:\n","        keypoints = {k: f[k][...] for k in f.keys()}\n","    return keypoints"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Load keypoints and visualize\n","keypoints = load_keypoints(feature_dir)\n","visualize_keypoints(dirname, keypoints)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["!ls"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["os.chdir(r'/kaggle/working')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["%cd /kaggle/working"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from IPython.display import FileLink\n","FileLink(r\"matches.h5\")"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":2058261,"sourceId":3414836,"sourceType":"datasetVersion"},{"datasetId":2069214,"sourceId":3462364,"sourceType":"datasetVersion"},{"datasetId":5210607,"sourceId":8689933,"sourceType":"datasetVersion"}],"dockerImageVersionId":30733,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
